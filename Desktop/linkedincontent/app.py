import streamlit as st
import requests
import pdfplumber
import io
import time
import textwrap
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.oxml.ns import qn

# --- å…¨å±€é…ç½® ---
st.set_page_config(page_title="Pro Research Agent (1:1 Exact Copy)", layout="wide", page_icon="ğŸ’")

# é…ç½®ç»˜å›¾é£æ ¼
plt.style.use('ggplot')
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial', 'DejaVu Sans', 'Microsoft YaHei'] 
plt.rcParams['axes.unicode_minus'] = False

# --- çŠ¶æ€ç®¡ç† ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'current_report' not in st.session_state:
    st.session_state['current_report'] = None

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def extract_pages_from_pdf(uploaded_file):
    """
    æŒ‰é¡µæå–æ–‡æœ¬ï¼Œè€Œä¸æ˜¯åˆå¹¶æˆä¸€å¤§å¨ã€‚
    è¿™æ˜¯ä¿è¯è¡¨æ ¼ä¸è¢«æ‰“æ–­ã€å†…å®¹ä¸ä¸¢å¤±çš„å…³é”®ã€‚
    """
    pages_content = []
    with pdfplumber.open(uploaded_file) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                # æ ‡è®°é¡µç ï¼Œå¸®åŠ© AI ç†è§£ä¸Šä¸‹æ–‡ï¼Œä½†è¦æ±‚ AI è¾“å‡ºæ—¶å»æ‰
                pages_content.append(text)
    return pages_content

def call_ai_api(api_key, base_url, model_name, messages, temperature=0.1, timeout=300):
    """
    æ¸©åº¦è®¾ä¸º 0.1ï¼Œå°½å¯èƒ½é™ä½ AI çš„åˆ›é€ æ€§ï¼Œå¼ºåˆ¶å®ƒåšâ€œå¤è¯»æœºâ€ä»¥ä¿è¯å†…å®¹ç²¾ç¡®ã€‚
    """
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model_name, "messages": messages, "temperature": temperature, "stream": False}
    try:
        response = requests.post(base_url, headers=headers, json=payload, timeout=timeout)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            print(f"âš ï¸ API Error: {response.status_code} - {response.text}")
            return None 
    except Exception as e:
        print(f"âš ï¸ Connection Error: {e}")
        return None

def create_professional_table_image(markdown_table_lines):
    """
    è¡¨æ ¼ç»˜å›¾å¼•æ“ï¼šä¿æŒåŸæœ‰é€»è¾‘ï¼Œç”Ÿæˆé«˜è´¨é‡è¡¨æ ¼å›¾ç‰‡
    """
    try:
        clean_rows = []
        for line in markdown_table_lines:
            content = line.strip().strip('|')
            # è¿‡æ»¤æ‰åˆ†å‰²çº¿è¡Œ (e.g. |---|---|)
            if not content or set(content.replace('|', '').strip()) <= {'-', ':', ' '}:
                continue
            clean_rows.append(line)

        if len(clean_rows) < 2: return None
        
        headers = [h.strip() for h in clean_rows[0].split('|') if h.strip()]
        if not headers: return None
        
        data = []
        row_heights = []
        col_width_chars = 25
        
        for row_line in clean_rows[1:]:
            raw_cells = [c.strip() for c in row_line.split('|') if c.strip() or c==""]
            # å¯¹é½åˆ—æ•°
            if len(raw_cells) > len(headers): raw_cells = raw_cells[:len(headers)]
            if len(raw_cells) < len(headers): raw_cells += [""] * (len(headers) - len(raw_cells))
            
            wrapped_row = []
            max_lines_in_row = 1
            
            for cell_text in raw_cells:
                wrapped_text = textwrap.fill(cell_text, width=col_width_chars, break_long_words=True)
                wrapped_row.append(wrapped_text)
                lines_count = wrapped_text.count('\n') + 1
                if lines_count > max_lines_in_row:
                    max_lines_in_row = lines_count
            
            data.append(wrapped_row)
            row_heights.append(max_lines_in_row)

        if not data: return None
        
        df = pd.DataFrame(data, columns=headers)

        base_row_height_inch = 0.45
        header_height_inch = 0.6
        total_data_height = sum([rh * base_row_height_inch for rh in row_heights])
        fig_height = header_height_inch + total_data_height + 0.5 # å¢åŠ ä¸€ç‚¹åº•éƒ¨padding
        fig_width = min(len(headers) * 2.8, 12) #ç¨å¾®åŠ å®½
        
        fig, ax = plt.subplots(figsize=(fig_width, fig_height))
        ax.axis('off')
        
        table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(11)
        
        cells = table.get_celld()
        for (row, col), cell in cells.items():
            cell.set_edgecolor('#d0d0d0')
            cell.set_linewidth(0.5)
            if row == 0:
                cell.set_height(header_height_inch / fig_height)
                cell.set_facecolor('#2c3e50')
                cell.set_text_props(color='white', weight='bold')
            else:
                height_multiplier = row_heights[row-1]
                cell.set_height((height_multiplier * base_row_height_inch) / fig_height)
                cell.set_facecolor('#f9f9f9' if row % 2 else '#ffffff')
                cell.set_text_props(color='#333333', ha='left')
                cell.set_text_props(position=(0.02, cell.get_text_props()['position'][1]))

        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', bbox_inches='tight', pad_inches=0.05, dpi=300)
        plt.close(fig)
        img_buffer.seek(0)
        return img_buffer

    except Exception as e:
        print(f"Table generation failed: {e}")
        return None

def generate_professional_word(content_text, model_name):
    doc = Document()
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Calibri'
    font.size = Pt(11)
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'SimHei')
    
    paragraph_format = style.paragraph_format
    paragraph_format.space_after = Pt(8)
    paragraph_format.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
    paragraph_format.line_spacing = 1.15
    paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY

    head = doc.add_heading('Investment Research Report', 0)
    head.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    meta = doc.add_paragraph(f"Original Content Transcribed by AI | {datetime.now().strftime('%Y-%m-%d')}")
    meta.alignment = WD_ALIGN_PARAGRAPH.RIGHT
    meta.runs[0].font.color.rgb = RGBColor(100, 100, 100)
    doc.add_paragraph("_" * 50)

    lines = content_text.split('\n')
    inside_table = False
    table_buffer = []

    for line in lines:
        stripped = line.strip()
        # åˆ¤å®šè¡¨æ ¼è¡Œçš„é€»è¾‘ä¼˜åŒ–ï¼šé¦–å°¾æœ‰|ï¼Œä¸”ä¸­é—´ä¹Ÿæœ‰|
        is_table_row = stripped.startswith('|') and stripped.endswith('|') and '|' in stripped[1:-1]
        
        if is_table_row:
            inside_table = True
            table_buffer.append(stripped)
        else:
            if inside_table:
                # è¡¨æ ¼ç»“æŸï¼Œå¼€å§‹ç»˜åˆ¶
                img = create_professional_table_image(table_buffer)
                if img: 
                    p = doc.add_paragraph()
                    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                    run = p.add_run()
                    run.add_picture(img, width=Inches(6.5)) # åŠ å®½å›¾ç‰‡
                # å³ä½¿ç”»å›¾å¤±è´¥ï¼Œä¹ŸæŠŠåŸå§‹Markdownè¡¨æ ¼æ–‡æœ¬å†™å…¥ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±
                else:
                    for tb_line in table_buffer:
                        doc.add_paragraph(tb_line, style='Normal')
                
                inside_table = False
                table_buffer = []
            
            if not stripped: continue
            
            # æ ‡é¢˜å¤„ç†
            if stripped.startswith('# '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 1)
                h.paragraph_format.space_before = Pt(18)
            elif stripped.startswith('## '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 2)
                h.paragraph_format.space_before = Pt(12)
            elif stripped.startswith('### '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 3)
            elif stripped.startswith('- ') or stripped.startswith('* '): 
                doc.add_paragraph(stripped[2:], style='List Bullet')
            else:
                doc.add_paragraph(stripped)

    # å¤„ç†æ–‡æ¡£æœ«å°¾å¯èƒ½çš„è¡¨æ ¼
    if inside_table and table_buffer:
        img = create_professional_table_image(table_buffer)
        if img: 
            p = doc.add_paragraph()
            p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = p.add_run()
            run.add_picture(img, width=Inches(6.5))
        else:
             for tb_line in table_buffer:
                doc.add_paragraph(tb_line, style='Normal')
    
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- UI ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ—ƒï¸ å†å²è®°å½•")
    if st.session_state['history']:
        for i, item in enumerate(reversed(st.session_state['history'])):
            if st.button(f"Load: {item['time']}", key=f"hist_{i}"):
                st.session_state['current_report'] = item
                st.rerun()
    
    st.divider()
    # é»˜è®¤ key å’Œ modelï¼Œå»ºè®®ä½¿ç”¨ä¸Šä¸‹æ–‡çª—å£å¤§çš„æ¨¡å‹
    api_key = st.text_input("API Key", value="sk-3UIO8MwTblfyQuEZz2WUCzQOuK4QwwIPALVcNxFFNUxJayu7", type="password")
    # å¼ºåŠ›æ¨èä½¿ç”¨ gemini-1.5-pro æˆ– gpt-4o æ¥å¤„ç†å¤æ‚æ ¼å¼
    model_name = st.selectbox("Model", ["gemini-3-pro", "gpt-4o", "qwen-max", "gemini-2.5-pro"])

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ’ Pro Research Agent (1:1 Perfect Copy)")
st.markdown("**Mode: Exact Transcription (Table Preservation)**")

uploaded_file = st.file_uploader("ä¸Šä¼  PDF èµ„æ–™ (å»ºè®®ä½¿ç”¨åŸç‰ˆPDFï¼Œéæ‰«æä»¶)", type=['pdf'])

if uploaded_file and st.button("ğŸ”¥ å¼€å§‹å®Œç¾è½¬åŒ–"):
    api_url = "https://api.nuwaapi.com/v1/chat/completions"
    
    # 1. è§£æ PDF (æŒ‰é¡µ)
    with st.spinner("ğŸ“– é€é¡µè¯»å– PDF..."):
        pages_list = extract_pages_from_pdf(uploaded_file)
        st.toast(f"å…±è¯†åˆ«åˆ° {len(pages_list)} é¡µï¼Œå¼€å§‹é€é¡µæ•°å­—åŒ–...")

    # 2. æ•°å­—åŒ– (Page-by-Page Processing)
    full_article_parts = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, page_text in enumerate(pages_list):
        status_text.markdown(f"**ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(pages_list)} é¡µ (ä¿ç•™è¡¨æ ¼ç»“æ„)...**")
        
        # --- æ ¸å¿ƒ Prompt ä¿®æ”¹ï¼šå¼ºåˆ¶ OCR æ¨¡å¼ ---
        prompt = """
        You are an advanced OCR and Formatting Engine. 
        Your Goal: Convert the provided text into PERFECT Markdown.
        
        STRICT EXECUTION RULES:
        1. **NO SUMMARIZATION**: You must output the text word-for-word. Do not delete any paragraphs.
        2. **TABLES ARE SACRED**: 
           - You MUST detect every table, even if it looks like a list.
           - You MUST output them as valid Markdown Tables (using | header | ... and |---| separator).
           - Do not skip numerical data.
        3. **FORMATTING**: Use # for headers, ## for subheaders, - for lists.
        4. **CLEANUP**: Remove page numbers like "Page 1 of 10" or footer dates.
        
        Input Text:
        """
        
        msg = [{"role": "user", "content": f"{prompt}\n\n{page_text}"}]
        
        page_res = None
        for attempt in range(3):
            # Temperature = 0.1 ç¡®ä¿ç²¾ç¡®å¤åˆ¶
            page_res = call_ai_api(api_key, api_url, model_name, msg, temperature=0.1)
            if page_res: 
                break
            time.sleep(2)
        
        if page_res:
            full_article_parts.append(page_res)
        else:
            print(f"âš ï¸ Page {i+1} failed. Falling back to raw text.")
            # å¦‚æœ AI å¤±è´¥ï¼Œç”¨ä»£ç å—åŒ…è£¹åŸå§‹æ–‡æœ¬ï¼Œæç¤ºç”¨æˆ·æ‰‹åŠ¨å¤„ç†
            fallback_content = f"\n\n> **[Page {i+1} Raw Text]**\n```\n{page_text}\n```\n\n" 
            full_article_parts.append(fallback_content)
            
        progress_bar.progress((i + 1) / len(pages_list))

    final_article = "\n\n".join(full_article_parts)
    status_text.success("âœ… 1:1 æ•°å­—åŒ–å®Œæˆï¼è¡¨æ ¼å·²é‡å»ºã€‚")

    # 3. ç”Ÿæˆ Word
    with st.spinner("ğŸ’¾ æ­£åœ¨æ¸²æŸ“ä¸“ä¸š Word (å«å›¾è¡¨)..."):
        word_bio = generate_professional_word(final_article, model_name)

    # 4. å­˜æ¡£
    report_data = {
        "time": datetime.now().strftime("%H:%M"),
        "filename": uploaded_file.name,
        "article": final_article,
        "word_data": word_bio.getvalue()
    }
    st.session_state['current_report'] = report_data
    st.session_state['history'].append(report_data)
    st.rerun()

# --- ç»“æœå±•ç¤º ---
current = st.session_state['current_report']

if current:
    st.divider()
    st.markdown(f"## ğŸ“Š äº¤ä»˜ç»“æœ: {current['filename']}")
    
    tab1, tab2 = st.tabs(["ğŸ“¥ Word ä¸‹è½½ & é¢„è§ˆ", "ğŸ“ çº¯ Markdown (ç”¨äºå¤åˆ¶)"])
    
    with tab1:
        col1, col2 = st.columns([3, 7])
        with col1:
            st.info("ğŸ‘‡ ç‚¹å‡»ä¸‹è½½åŒ…å«å®Œç¾è¡¨æ ¼çš„ Word æ–‡æ¡£")
            st.download_button(
                "ğŸ“¥ ä¸‹è½½ä¸“ä¸š Word æŠ¥å‘Š",
                data=current['word_data'],
                file_name=f"Pro_Report_{current['time']}.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
        with col2:
            st.markdown("### ğŸ“„ æ¸²æŸ“æ•ˆæœé¢„è§ˆ")
            # è¿™é‡Œä½¿ç”¨ st.markdown æ¸²æŸ“ï¼Œå¯ä»¥çœ‹åˆ°è¡¨æ ¼æ•ˆæœ
            st.markdown(current['article'])

    with tab2:
        st.warning("æç¤ºï¼šç‚¹å‡»å³ä¸Šè§’å¤åˆ¶æŒ‰é’®ï¼Œå³å¯è·å¾—å¸¦æ ¼å¼çš„çº¯æ–‡æœ¬ï¼ˆå« Markdown è¡¨æ ¼æºç ï¼‰")
        st.code(current['article'], language="markdown")

elif not uploaded_file:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼  PDF æ–‡ä»¶ã€‚æœ¬æ¨¡å¼å°†å¼€å¯â€˜OCRçº§â€™é€é¡µç²¾ç»†å¤„ç†ï¼Œç¡®ä¿è¡¨æ ¼å’Œå…¨æ–‡å†…å®¹ 100% å®Œæ•´ã€‚")
