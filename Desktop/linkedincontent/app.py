import streamlit as st
import requests
import pdfplumber
import io
import time
import textwrap
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.oxml.ns import qn

# --- å…¨å±€é…ç½® ---
st.set_page_config(page_title="Pro Research Agent", layout="wide", page_icon="ğŸ’")

# é…ç½®ç»˜å›¾é£æ ¼
plt.style.use('ggplot')
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial', 'DejaVu Sans', 'Microsoft YaHei'] 
plt.rcParams['axes.unicode_minus'] = False

# --- çŠ¶æ€ç®¡ç† ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'current_report' not in st.session_state:
    st.session_state['current_report'] = None

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def extract_text_from_pdf(uploaded_file):
    text = ""
    with pdfplumber.open(uploaded_file) as pdf:
        for i, page in enumerate(pdf.pages):
            page_text = page.extract_text()
            if page_text:
                text += f"\n\n{page_text}" 
    return text

def split_text_into_chunks(text, chunk_size=2500):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def call_ai_api(api_key, base_url, model_name, messages, temperature=0.3, timeout=300):
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model_name, "messages": messages, "temperature": temperature, "stream": False}
    try:
        response = requests.post(base_url, headers=headers, json=payload, timeout=timeout)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            # æ‰“å°é”™è¯¯ä½†ä¸ä¸­æ–­ï¼Œæ–¹ä¾¿ fallback æ¥ç®¡
            print(f"âš ï¸ API Error: {response.status_code}")
            return None 
    except Exception as e:
        print(f"âš ï¸ Connection Error: {e}")
        return None

def create_professional_table_image(markdown_table_lines):
    """
    ã€ä¿ç•™ä½ æ»¡æ„çš„ç‰ˆæœ¬ã€‘è¡¨æ ¼ç»˜å›¾å¼•æ“ï¼šåŠ¨æ€è¡Œé«˜ + é›¶ç™½è¾¹
    """
    try:
        clean_rows = []
        for line in markdown_table_lines:
            content = line.strip().strip('|')
            if not content or set(content.replace('|', '').strip()) <= {'-', ':', ' '}:
                continue
            clean_rows.append(line)

        if len(clean_rows) < 2: return None
        
        headers = [h.strip() for h in clean_rows[0].split('|') if h.strip()]
        if not headers: return None
        
        data = []
        row_heights = []
        col_width_chars = 25
        
        for row_line in clean_rows[1:]:
            raw_cells = [c.strip() for c in row_line.split('|') if c.strip() or c==""]
            if len(raw_cells) > len(headers): raw_cells = raw_cells[:len(headers)]
            if len(raw_cells) < len(headers): raw_cells += [""] * (len(headers) - len(raw_cells))
            
            wrapped_row = []
            max_lines_in_row = 1
            
            for cell_text in raw_cells:
                wrapped_text = textwrap.fill(cell_text, width=col_width_chars, break_long_words=True)
                wrapped_row.append(wrapped_text)
                lines_count = wrapped_text.count('\n') + 1
                if lines_count > max_lines_in_row:
                    max_lines_in_row = lines_count
            
            data.append(wrapped_row)
            row_heights.append(max_lines_in_row)

        if not data: return None
        
        df = pd.DataFrame(data, columns=headers)

        base_row_height_inch = 0.45
        header_height_inch = 0.6
        total_data_height = sum([rh * base_row_height_inch for rh in row_heights])
        fig_height = header_height_inch + total_data_height
        fig_width = min(len(headers) * 2.5, 11)
        
        fig, ax = plt.subplots(figsize=(fig_width, fig_height))
        ax.axis('off')
        
        table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(11)
        
        cells = table.get_celld()
        for (row, col), cell in cells.items():
            cell.set_edgecolor('#d0d0d0')
            cell.set_linewidth(0.5)
            if row == 0:
                cell.set_height(header_height_inch / fig_height)
                cell.set_facecolor('#2c3e50')
                cell.set_text_props(color='white', weight='bold')
            else:
                height_multiplier = row_heights[row-1]
                cell.set_height((height_multiplier * base_row_height_inch) / fig_height)
                cell.set_facecolor('#f9f9f9' if row % 2 else '#ffffff')
                cell.set_text_props(color='#333333', ha='left')
                cell.set_text_props(position=(0.02, cell.get_text_props()['position'][1]))

        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', bbox_inches='tight', pad_inches=0.02, dpi=300)
        plt.close(fig)
        img_buffer.seek(0)
        return img_buffer

    except Exception:
        return None

def generate_professional_word(content_text, model_name):
    """
    ã€ä¿ç•™ä½ æ»¡æ„çš„ç‰ˆæœ¬ã€‘Word ç”Ÿæˆé€»è¾‘
    """
    doc = Document()
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Calibri'
    font.size = Pt(11)
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'SimHei')
    
    paragraph_format = style.paragraph_format
    paragraph_format.space_after = Pt(8)
    paragraph_format.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
    paragraph_format.line_spacing = 1.15
    paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY

    head = doc.add_heading('Investment Research Report', 0)
    head.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    meta = doc.add_paragraph(f"Generated by AI Agent | {datetime.now().strftime('%Y-%m-%d')}")
    meta.alignment = WD_ALIGN_PARAGRAPH.RIGHT
    meta.runs[0].font.color.rgb = RGBColor(100, 100, 100)
    doc.add_paragraph("_" * 50)

    lines = content_text.split('\n')
    inside_table = False
    table_buffer = []

    for line in lines:
        stripped = line.strip()
        is_table_row = stripped.startswith('|') and stripped.endswith('|')
        
        if is_table_row:
            inside_table = True
            table_buffer.append(stripped)
        else:
            if inside_table:
                img = create_professional_table_image(table_buffer)
                if img: 
                    p = doc.add_paragraph()
                    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                    run = p.add_run()
                    run.add_picture(img, width=Inches(6.2))
                inside_table = False
                table_buffer = []
            
            if not stripped: continue
            
            if stripped.startswith('# '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 1)
                h.paragraph_format.space_before = Pt(18)
            elif stripped.startswith('## '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 2)
                h.paragraph_format.space_before = Pt(12)
            elif stripped.startswith('### '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 3)
            elif stripped.startswith('- ') or stripped.startswith('* '): 
                doc.add_paragraph(stripped[2:], style='List Bullet')
            else:
                doc.add_paragraph(stripped)

    if inside_table and table_buffer:
        img = create_professional_table_image(table_buffer)
        if img: 
            p = doc.add_paragraph()
            p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = p.add_run()
            run.add_picture(img, width=Inches(6.2))
    
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- UI ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ—ƒï¸ å†å²è®°å½•")
    if st.session_state['history']:
        for i, item in enumerate(reversed(st.session_state['history'])):
            if st.button(f"Load: {item['time']}", key=f"hist_{i}"):
                st.session_state['current_report'] = item
                st.rerun()
    
    st.divider()
    api_key = st.text_input("API Key", value="sk-3UIO8MwTblfyQuEZz2WUCzQOuK4QwwIPALVcNxFFNUxJayu7", type="password")
    model_name = st.selectbox("Model", ["gemini-3-pro", "gemini-2.5-pro", "qwen-max", "gpt-4o"])

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ’ Pro Research Agent (Final Stable)")

uploaded_file = st.file_uploader("ä¸Šä¼  PDF èµ„æ–™", type=['pdf'])

if uploaded_file and st.button("ğŸ”¥ å¼€å§‹å®Œç¾è½¬åŒ–"):
    api_url = "https://api.nuwaapi.com/v1/chat/completions"
    
    # 1. è§£æ PDF
    with st.spinner("ğŸ“– è¯»å– PDF..."):
        raw_text = extract_text_from_pdf(uploaded_file)

    # 2. æ•°å­—åŒ– (1:1 æ ¼å¼åŒ–) - ã€æ ¸å¿ƒä¿®å¤åŒºåŸŸã€‘
    chunks = split_text_into_chunks(raw_text, chunk_size=2500)
    full_article_parts = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, chunk in enumerate(chunks):
        status_text.markdown(f"**ğŸ”„ æ ¼å¼åŒ–å¤„ç†ä¸­: Part {i+1}/{len(chunks)}**")
        
        prompt = """
        You are a Senior Data Entry Specialist. 
        Task: Digitally transcribe the provided document text into Markdown.
        
        **STRICT RULES**:
        1. **TABLES**: Merge cross-page tables into one. Output valid Markdown tables (|...|).
        2. **CONTENT**: Word-for-word transcription. No summarization.
        """
        msg = [{"role": "user", "content": f"{prompt}\n\nRAW CONTENT:\n{chunk}"}]
        
        chunk_res = None
        # é‡è¯•æ¬¡æ•°å¢åŠ åˆ° 3 æ¬¡
        for attempt in range(3):
            chunk_res = call_ai_api(api_key, api_url, model_name, msg)
            if chunk_res: 
                break
            # æŒ‡æ•°é€€é¿ï¼šå¤±è´¥ä¸€æ¬¡ç­‰å¾…æ—¶é—´åŠ é•¿ (1s, 2s, 4s)
            time.sleep(2 ** attempt)
        
        if chunk_res:
            full_article_parts.append(chunk_res)
        else:
            # === æ ¸å¿ƒä¿®å¤ï¼šä¿åº•æœºåˆ¶ (Fallback) ===
            # å¦‚æœ AI å½»åº•å¤±è´¥ï¼Œç›´æ¥å¡«å…¥åŸå§‹æ–‡æœ¬ï¼Œç»ä¸æ˜¾ç¤º Error processing
            print(f"âš ï¸ Part {i+1} failed AI formatting. Falling back to raw text.")
            fallback_content = f"\n\n{chunk}\n\n" # ä½¿ç”¨åŸå§‹ OCR æ–‡æœ¬
            full_article_parts.append(fallback_content)
            
        progress_bar.progress((i + 1) / len(chunks))
        # å¼ºåˆ¶å†·å´ï¼šæ¯æ¬¡æˆåŠŸå¤„ç†åä¼‘æ¯ 2 ç§’ï¼Œé˜²æ­¢é€Ÿç‡é™åˆ¶
        time.sleep(2)

    final_article = "\n\n".join(full_article_parts)
    status_text.success("âœ… æ ¼å¼åŒ–å®Œæˆï¼")

    # 3. ç¤¾åª’ç”Ÿæˆ (ä¸‰çº§é‡è¯•)
    with st.spinner("ğŸ§  æ­£åœ¨æ’°å†™æ·±åº¦ç¤¾åª’..."):
        social_res = None
        
        context_head = final_article[:5000]
        context_tail = final_article[-5000:] if len(final_article) > 5000 else ""
        social_context_full = context_head + "\n\n[...SKIPPING...]\n\n" + context_tail
        
        social_prompt = """
        Act as a Lead Analyst at a Hedge Fund. Write social media content.
        **GOAL**: Sell the *Logic*, *Catalysts*, and *Upside*. 
        **PLATFORMS**: LinkedIn, Twitter (Thread), Reddit (DD style), Xiaohongshu.
        Split with '==='.
        """
        
        # å°è¯• 1
        msg_social = [{"role": "user", "content": f"{social_prompt}\n\nREPORT:\n{social_context_full}"}]
        social_res = call_ai_api(api_key, api_url, model_name, msg_social, temperature=0.7, timeout=120)
        
        # å°è¯• 2
        if not social_res:
            short_context = final_article[:3000] + "\n...\n" + final_article[-3000:]
            msg_social_short = [{"role": "user", "content": f"{social_prompt}\n\nREPORT:\n{short_context}"}]
            social_res = call_ai_api(api_key, api_url, model_name, msg_social_short, temperature=0.7, timeout=120)

        # å°è¯• 3
        if not social_res:
            minimal_context = final_article[:3000]
            msg_social_min = [{"role": "user", "content": f"{social_prompt}\n\nREPORT START:\n{minimal_context}"}]
            social_res = call_ai_api(api_key, api_url, model_name, msg_social_min, temperature=0.7, timeout=60)

        if not social_res: 
            social_res = "âš ï¸ ç¤¾åª’ç”Ÿæˆå¤±è´¥ã€‚è¯·æ£€æŸ¥ API è¿æ¥ã€‚"

    # 4. ç”Ÿæˆ Word
    with st.spinner("ğŸ’¾ æ­£åœ¨æ¸²æŸ“ä¸“ä¸š Word æ–‡æ¡£..."):
        word_bio = generate_professional_word(final_article, model_name)

    # 5. å­˜æ¡£
    report_data = {
        "time": datetime.now().strftime("%H:%M"),
        "filename": uploaded_file.name,
        "article": final_article,
        "social": social_res,
        "word_data": word_bio.getvalue()
    }
    st.session_state['current_report'] = report_data
    st.session_state['history'].append(report_data)
    st.rerun()

# --- ç»“æœå±•ç¤º ---
current = st.session_state['current_report']

if current:
    st.divider()
    st.markdown(f"## ğŸ“Š äº¤ä»˜: {current['filename']}")
    col1, col2 = st.columns([5, 5])
    
    with col1:
        st.download_button(
            "ğŸ“¥ ä¸‹è½½ Word",
            data=current['word_data'],
            file_name=f"Pro_Report_{current['time']}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        with st.expander("ğŸ“„ åŸå§‹å†…å®¹é¢„è§ˆ", expanded=False):
            st.markdown(current['article'])

    with col2:
        if "âš ï¸" in str(current['social']):
             st.warning("éƒ¨åˆ†ç¤¾åª’å†…å®¹ç”Ÿæˆé‡åˆ°å»¶è¿Ÿ")
        else:
             st.success("ğŸ”¥ æ·±åº¦ç¤¾åª’æ–‡æ¡ˆ")
        
        st.text_area("Copy", value=current['social'], height=800)

elif not uploaded_file:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼ æ–‡ä»¶ã€‚å·²å¯ç”¨ API ä¿æŠ¤ä¸è‡ªåŠ¨é™çº§æœºåˆ¶ï¼Œæœç» Error æŠ¥é”™ã€‚")
