import streamlit as st
import requests
import pdfplumber
import io
import time
import textwrap
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.oxml.ns import qn

# --- å…¨å±€é…ç½® ---
st.set_page_config(page_title="Pro Research Agent", layout="wide", page_icon="ğŸ’")

# é…ç½®ç»˜å›¾é£æ ¼ (æ”¯æŒä¸­æ–‡å’Œç‰¹æ®Šç¬¦å·)
plt.style.use('ggplot')
plt.rcParams['font.family'] = 'sans-serif'
# å°è¯•å¤šç§å­—ä½“ä»¥é€‚é…ä¸åŒæœåŠ¡å™¨ç¯å¢ƒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial', 'DejaVu Sans', 'Microsoft YaHei'] 
plt.rcParams['axes.unicode_minus'] = False

# --- çŠ¶æ€ç®¡ç† ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'current_report' not in st.session_state:
    st.session_state['current_report'] = None

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def extract_text_from_pdf(uploaded_file):
    """
    æå–PDFæ–‡æœ¬ã€‚ä¸æŒ‰é¡µå¼ºè¡Œåˆ†å‰²ï¼Œè€Œæ˜¯æä¾›æµå¼æ–‡æœ¬ï¼Œ
    æœ‰åŠ©äºæ¨¡å‹ç†è§£è·¨é¡µè¡¨æ ¼ã€‚
    """
    text = ""
    with pdfplumber.open(uploaded_file) as pdf:
        for i, page in enumerate(pdf.pages):
            page_text = page.extract_text()
            if page_text:
                text += f"\n\n{page_text}" 
    return text

def split_text_into_chunks(text, chunk_size=2500):
    """åˆ‡åˆ†é•¿æ–‡æœ¬"""
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def call_ai_api(api_key, base_url, model_name, messages, temperature=0.3, timeout=300):
    """
    å¢å¼ºç‰ˆ API è°ƒç”¨ï¼šæ”¯æŒè‡ªå®šä¹‰è¶…æ—¶ï¼Œè¿”å›è¯¦ç»†é”™è¯¯
    """
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model_name, "messages": messages, "temperature": temperature, "stream": False}
    try:
        response = requests.post(base_url, headers=headers, json=payload, timeout=timeout)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            print(f"âš ï¸ API Error: {response.status_code} - {response.text[:100]}")
            return None 
    except Exception as e:
        print(f"âš ï¸ Connection Error: {e}")
        return None

def create_professional_table_image(markdown_table_lines):
    """
    ã€ç»ˆæç‰ˆã€‘è¡¨æ ¼ç»˜å›¾å¼•æ“ï¼š
    1. åŠ¨æ€è¡Œé«˜è®¡ç®—ï¼šæ ¹æ®æ–‡å­—é‡è‡ªåŠ¨æ’‘å¼€å•å…ƒæ ¼ï¼Œæœç»é‡å ã€‚
    2. é›¶ç™½è¾¹ï¼šå›¾ç‰‡ç´§è´´è¡¨æ ¼è¾¹ç¼˜ã€‚
    3. å¼ºåŠ›æ¸…æ´—ï¼šè¿‡æ»¤ Markdown åˆ†éš”ç¬¦ã€‚
    """
    try:
        # --- 1. æ•°æ®æ¸…æ´—ä¸è§£æ ---
        clean_rows = []
        for line in markdown_table_lines:
            content = line.strip().strip('|')
            # è¿‡æ»¤æ‰åªåŒ…å«åˆ†éš”ç¬¦(-, :, |)çš„è¡Œ
            if not content or set(content.replace('|', '').strip()) <= {'-', ':', ' '}:
                continue
            clean_rows.append(line)

        if len(clean_rows) < 2: return None
        
        # æå–è¡¨å¤´
        headers = [h.strip() for h in clean_rows[0].split('|') if h.strip()]
        if not headers: return None
        
        # æå–æ•°æ®å¹¶é¢„å¤„ç†
        data = []
        row_heights = [] # è®°å½•æ¯ä¸€è¡Œéœ€è¦çš„å€æ•°é«˜åº¦
        col_width_chars = 25 # è®¾å®šæ¯åˆ—å¤§çº¦å¤šå°‘å­—ç¬¦æ¢è¡Œ
        
        for row_line in clean_rows[1:]:
            raw_cells = [c.strip() for c in row_line.split('|') if c.strip() or c==""]
            
            # å¯¹é½åˆ—æ•°
            if len(raw_cells) > len(headers): raw_cells = raw_cells[:len(headers)]
            if len(raw_cells) < len(headers): raw_cells += [""] * (len(headers) - len(raw_cells))
            
            wrapped_row = []
            max_lines_in_row = 1
            
            for cell_text in raw_cells:
                # å¼ºåˆ¶æ¢è¡Œå¤„ç†
                wrapped_text = textwrap.fill(cell_text, width=col_width_chars, break_long_words=True)
                wrapped_row.append(wrapped_text)
                
                # è®¡ç®—è¯¥å•å…ƒæ ¼å ç”¨çš„è¡Œæ•°
                lines_count = wrapped_text.count('\n') + 1
                if lines_count > max_lines_in_row:
                    max_lines_in_row = lines_count
            
            data.append(wrapped_row)
            row_heights.append(max_lines_in_row)

        if not data: return None
        
        df = pd.DataFrame(data, columns=headers)

        # --- 2. åŠ¨æ€è®¡ç®—å›¾ç‰‡å°ºå¯¸ ---
        base_row_height_inch = 0.45 # åŸºç¡€è¡Œé«˜
        header_height_inch = 0.6    # è¡¨å¤´é«˜åº¦
        
        # æ€»é«˜åº¦ = è¡¨å¤´ + æ‰€æœ‰æ•°æ®è¡Œçš„é«˜åº¦å’Œ
        total_data_height = sum([rh * base_row_height_inch for rh in row_heights])
        fig_height = header_height_inch + total_data_height
        
        # æ€»å®½åº¦
        fig_width = min(len(headers) * 2.5, 11) # é™åˆ¶æœ€å¤§å®½åº¦
        
        fig, ax = plt.subplots(figsize=(fig_width, fig_height))
        ax.axis('off')
        
        # --- 3. ç»˜åˆ¶è¡¨æ ¼ ---
        table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(11)
        
        # --- 4. æ ·å¼ç²¾ä¿® ---
        cells = table.get_celld()
        
        for (row, col), cell in cells.items():
            cell.set_edgecolor('#d0d0d0')
            cell.set_linewidth(0.5)
            
            if row == 0:
                # è¡¨å¤´æ ·å¼
                cell.set_height(header_height_inch / fig_height)
                cell.set_facecolor('#2c3e50')
                cell.set_text_props(color='white', weight='bold')
            else:
                # æ•°æ®è¡Œæ ·å¼
                height_multiplier = row_heights[row-1]
                # è®¾ç½®è¯¥è¡Œé«˜åº¦æ¯”ä¾‹
                cell.set_height((height_multiplier * base_row_height_inch) / fig_height)
                
                cell.set_facecolor('#f9f9f9' if row % 2 else '#ffffff')
                cell.set_text_props(color='#333333')
                # å·¦å¯¹é½å¹¶å¢åŠ å†…è¾¹è·
                cell.set_text_props(ha='left')
                cell.set_text_props(position=(0.02, cell.get_text_props()['position'][1]))

        # --- 5. ä¿å­˜å›¾ç‰‡ (å»ç™½è¾¹) ---
        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', bbox_inches='tight', pad_inches=0.02, dpi=300)
        plt.close(fig)
        img_buffer.seek(0)
        return img_buffer

    except Exception as e:
        print(f"Table Render Failed: {e}")
        return None

def generate_professional_word(content_text, model_name):
    """
    ç”Ÿæˆ MBB å’¨è¯¢çº§ Word æ–‡æ¡£
    """
    doc = Document()
    
    # å…¨å±€æ ·å¼è®¾ç½®
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Calibri'
    font.size = Pt(11)
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'SimHei')
    
    # æ®µè½æ ¼å¼
    paragraph_format = style.paragraph_format
    paragraph_format.space_after = Pt(8)
    paragraph_format.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
    paragraph_format.line_spacing = 1.15
    paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY

    # æŠ¬å¤´
    head = doc.add_heading('Investment Research Report', 0)
    head.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    meta = doc.add_paragraph(f"Generated by AI Agent | {datetime.now().strftime('%Y-%m-%d')}")
    meta.alignment = WD_ALIGN_PARAGRAPH.RIGHT
    meta.runs[0].font.color.rgb = RGBColor(100, 100, 100)
    doc.add_paragraph("_" * 50)

    lines = content_text.split('\n')
    inside_table = False
    table_buffer = []

    for line in lines:
        stripped = line.strip()
        is_table_row = stripped.startswith('|') and stripped.endswith('|')
        
        if is_table_row:
            inside_table = True
            table_buffer.append(stripped)
        else:
            # è¡¨æ ¼æ¸²æŸ“é€»è¾‘
            if inside_table:
                img = create_professional_table_image(table_buffer)
                if img: 
                    p = doc.add_paragraph()
                    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                    run = p.add_run()
                    run.add_picture(img, width=Inches(6.2)) # é€‚åº”A4é¡µé¢å®½åº¦
                inside_table = False
                table_buffer = []
            
            if not stripped: continue
            
            # æ ‡é¢˜ä¸æ­£æ–‡æ¸²æŸ“
            if stripped.startswith('# '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 1)
                h.paragraph_format.space_before = Pt(18)
            elif stripped.startswith('## '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 2)
                h.paragraph_format.space_before = Pt(12)
            elif stripped.startswith('### '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 3)
            elif stripped.startswith('- ') or stripped.startswith('* '): 
                doc.add_paragraph(stripped[2:], style='List Bullet')
            else:
                doc.add_paragraph(stripped)

    # å¤„ç†æ–‡æœ«æ®‹ç•™è¡¨æ ¼
    if inside_table and table_buffer:
        img = create_professional_table_image(table_buffer)
        if img: 
            p = doc.add_paragraph()
            p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = p.add_run()
            run.add_picture(img, width=Inches(6.2))
    
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- UI ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ—ƒï¸ å†å²è®°å½•")
    if st.session_state['history']:
        for i, item in enumerate(reversed(st.session_state['history'])):
            if st.button(f"Load: {item['time']}", key=f"hist_{i}"):
                st.session_state['current_report'] = item
                st.rerun()
    else:
        st.caption("æš‚æ— è®°å½•")
    
    st.divider()
    api_key = st.text_input("API Key", value="sk-3UIO8MwTblfyQuEZz2WUCzQOuK4QwwIPALVcNxFFNUxJayu7", type="password")
    model_name = st.selectbox("Model", ["gemini-3-pro", "gemini-2.5-pro", "qwen-max", "gpt-4o"])

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ’ Pro Research Agent (Final Ver.)")
st.caption("MBB-Style Reports | Visualized Tables | Deep Reddit DD")

uploaded_file = st.file_uploader("ä¸Šä¼  PDF èµ„æ–™", type=['pdf'])

if uploaded_file and st.button("ğŸ”¥ å¼€å§‹å®Œç¾è½¬åŒ–"):
    api_url = "https://api.nuwaapi.com/v1/chat/completions"
    
    # 1. è§£æ PDF
    with st.spinner("ğŸ“– è¯»å– PDF..."):
        raw_text = extract_text_from_pdf(uploaded_file)

    # 2. æ•°å­—åŒ– (1:1 æ ¼å¼åŒ–)
    chunks = split_text_into_chunks(raw_text, chunk_size=2500)
    full_article_parts = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, chunk in enumerate(chunks):
        status_text.markdown(f"**ğŸ”„ æ ¼å¼åŒ–å¤„ç†ä¸­: Part {i+1}/{len(chunks)}**")
        
        prompt = """
        You are a Senior Data Entry Specialist. 
        Task: Digitally transcribe the provided document text into Markdown.
        
        **STRICT RULES**:
        1. **TABLES**: Merge cross-page tables into one. Output valid Markdown tables (|...|).
        2. **CONTENT**: Word-for-word transcription. No summarization.
        3. **FORMAT**: Keep headers and lists structure.
        """
        msg = [{"role": "user", "content": f"{prompt}\n\nRAW CONTENT:\n{chunk}"}]
        
        chunk_res = None
        for attempt in range(2):
            chunk_res = call_ai_api(api_key, api_url, model_name, msg)
            if chunk_res: break
            time.sleep(1)
        
        if chunk_res:
            full_article_parts.append(chunk_res)
        else:
            full_article_parts.append(f"\n\n[Error processing part {i+1}]\n\n")
            
        progress_bar.progress((i + 1) / len(chunks))

    final_article = "\n\n".join(full_article_parts)
    status_text.success("âœ… æ ¼å¼åŒ–å®Œæˆï¼")

    # 3. ç¤¾åª’ç”Ÿæˆ (å¸¦é‡è¯•æœºåˆ¶)
    with st.spinner("ğŸ§  æ­£åœ¨æ’°å†™æ·±åº¦ç¤¾åª’ (Retrying enabled)..."):
        
        social_res = None
        
        # ä¸Šä¸‹æ–‡ç­–ç•¥ 1: å®Œæ•´ä¸Šä¸‹æ–‡
        context_head = final_article[:5000]
        context_tail = final_article[-5000:] if len(final_article) > 5000 else ""
        social_context_full = context_head + "\n\n[...SKIPPING...]\n\n" + context_tail
        
        social_prompt = """
        Act as a Lead Analyst at a Hedge Fund. Write social media content.
        
        **CORE GOAL**: Sell the *Logic*, *Catalysts*, and *Upside*. 
        **DO NOT** write a summary. Write an **INVESTMENT THESIS**.
        
        **PLATFORMS**:
        
        ### ğŸ”µ LinkedIn
        - Professional analysis of the Moat/Strategy.
        
        ### âš« Twitter (Thread)
        - Hook with a shocking number.
        - 5 Tweets on "Asymmetric Upside".
        
        ### ğŸ”´ Reddit (r/SecurityAnalysis Style DD)
        - **Title**: [DD] [Ticker] - The Bull/Bear Case (Deep Dive)
        - **Structure**: TL;DR -> The Thesis -> The Numbers -> The Risks -> Conclusion.
        - **Tone**: Objective, analytical, hard-core.
        
        ### ğŸŸ  Xiaohongshu
        - Title: âš ï¸è®¤çŸ¥å·®ï¼çœŸæ­£çš„çˆ†å‘é€»è¾‘
        - Focus on: Catalyst & Next Big Thing.
        
        Split with '==='.
        """
        
        # å°è¯• 1
        msg_social = [{"role": "user", "content": f"{social_prompt}\n\nREPORT:\n{social_context_full}"}]
        social_res = call_ai_api(api_key, api_url, model_name, msg_social, temperature=0.7, timeout=120)
        
        # å°è¯• 2 (ç¼©å‡ä¸Šä¸‹æ–‡)
        if not social_res:
            print("Retry 1: Reducing context size...")
            short_context = final_article[:3000] + "\n...\n" + final_article[-3000:]
            msg_social_short = [{"role": "user", "content": f"{social_prompt}\n\nREPORT:\n{short_context}"}]
            social_res = call_ai_api(api_key, api_url, model_name, msg_social_short, temperature=0.7, timeout=120)

        # å°è¯• 3 (æç®€)
        if not social_res:
            print("Retry 2: Minimal context...")
            minimal_context = final_article[:3000]
            msg_social_min = [{"role": "user", "content": f"{social_prompt}\n\nREPORT START:\n{minimal_context}"}]
            social_res = call_ai_api(api_key, api_url, model_name, msg_social_min, temperature=0.7, timeout=60)

        if not social_res: 
            social_res = "âš ï¸ ç¤¾åª’ç”Ÿæˆå¤±è´¥ã€‚è¯·æ£€æŸ¥ API è¿æ¥æˆ–ç¨åé‡è¯•ã€‚"

    # 4. ç”Ÿæˆ Word
    with st.spinner("ğŸ’¾ æ­£åœ¨æ¸²æŸ“ä¸“ä¸š Word æ–‡æ¡£..."):
        word_bio = generate_professional_word(final_article, model_name)

    # 5. å­˜æ¡£
    report_data = {
        "time": datetime.now().strftime("%H:%M"),
        "filename": uploaded_file.name,
        "article": final_article,
        "social": social_res,
        "word_data": word_bio.getvalue()
    }
    st.session_state['current_report'] = report_data
    st.session_state['history'].append(report_data)
    st.rerun()

# --- ç»“æœå±•ç¤º ---
current = st.session_state['current_report']

if current:
    st.divider()
    st.markdown(f"## ğŸ“Š äº¤ä»˜: {current['filename']}")
    col1, col2 = st.columns([5, 5])
    
    with col1:
        st.download_button(
            "ğŸ“¥ ä¸‹è½½ Word (å’¨è¯¢çº§æ’ç‰ˆ+é«˜æ¸…å›¾è¡¨)",
            data=current['word_data'],
            file_name=f"Pro_Report_{current['time']}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        with st.expander("ğŸ“„ åŸå§‹å†…å®¹é¢„è§ˆ", expanded=False):
            st.markdown(current['article'])

    with col2:
        if "âš ï¸" in current['social']:
             st.error("ç¤¾åª’ç”Ÿæˆéƒ¨åˆ†å¤±è´¥")
        else:
             st.success("ğŸ”¥ æ·±åº¦ç¤¾åª’æ–‡æ¡ˆ (Reddit DD & Insight)")
        
        st.text_area("Copy", value=current['social'], height=800)

elif not uploaded_file:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼ æ–‡ä»¶ã€‚ç³»ç»Ÿå°†æ‰§è¡Œå®Œç¾å¤åˆ»ä¸æ·±åº¦åˆ†æã€‚")
