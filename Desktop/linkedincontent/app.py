import streamlit as st
import requests
import pdfplumber
import io
import time
import textwrap
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.oxml.ns import qn

# --- å…¨å±€é…ç½® ---
st.set_page_config(page_title="Pro Research Agent", layout="wide", page_icon="ğŸ’")
# é…ç½®ä¸“ä¸šç»˜å›¾é£æ ¼
plt.style.use('ggplot')
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial', 'DejaVu Sans'] 

# --- çŠ¶æ€ç®¡ç† ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'current_report' not in st.session_state:
    st.session_state['current_report'] = None

# --- æ ¸å¿ƒå‡½æ•° ---

def extract_text_from_pdf(uploaded_file):
    """
    æå–æ–‡æœ¬ï¼Œå¹¶ä¸å†æœºæ¢°åœ°æŒ‰é¡µåˆ†å‰²ï¼Œè€Œæ˜¯å°è¯•ä»¥æµå¼æ–‡æœ¬æä¾›ï¼Œ
    æœ‰åŠ©äºè§£å†³è·¨é¡µè¡¨æ ¼æ–­è£‚çš„é—®é¢˜ã€‚
    """
    text = ""
    with pdfplumber.open(uploaded_file) as pdf:
        for i, page in enumerate(pdf.pages):
            page_text = page.extract_text()
            if page_text:
                # å»æ‰é¡µè„šé¡µçœ‰çš„å¹²æ‰°ï¼ˆç®€å•è§„åˆ™ï¼‰ï¼Œåªä¿ç•™æ ¸å¿ƒå†…å®¹
                text += f"\n\n{page_text}" 
    return text

def split_text_into_chunks(text, chunk_size=2500):
    # ç¨å¾®åŠ å¤§ Chunkï¼Œè®©è¡¨æ ¼å°½å¯èƒ½åœ¨ä¸€ä¸ªå—é‡Œ
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

def call_ai_api(api_key, base_url, model_name, messages, temperature=0.3):
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model_name, "messages": messages, "temperature": temperature, "stream": False}
    try:
        response = requests.post(base_url, headers=headers, json=payload, timeout=300)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        return None 
    except:
        return None

def create_professional_table_image(markdown_table_lines):
    """
    ã€å‡çº§ç‰ˆã€‘è¡¨æ ¼ç»˜å›¾å¼•æ“ï¼šæ”¯æŒè‡ªåŠ¨æ¢è¡Œã€ä¸“ä¸šé…è‰²ã€å®Œæ•´æ˜¾ç¤º
    """
    try:
        # 1. æ¸…æ´—æ•°æ®
        clean_rows = [line for line in markdown_table_lines if not set(line.replace('|', '').strip()) == {'-'}]
        if len(clean_rows) < 2: return None
        
        headers = [h.strip() for h in clean_rows[0].split('|') if h.strip()]
        data = []
        for row in clean_rows[1:]:
            row_data = [c.strip() for c in row.split('|') if c.strip() or c==""]
            # å¯¹é½å¤„ç†
            if len(row_data) > len(headers): row_data = row_data[:len(headers)]
            if len(row_data) < len(headers): row_data += [""] * (len(headers) - len(row_data))
            
            # ã€å…³é”®ã€‘å¯¹æ¯ä¸ªå•å…ƒæ ¼è¿›è¡Œè‡ªåŠ¨æ¢è¡Œå¤„ç†ï¼Œé˜²æ­¢å›¾ç‰‡è¿‡å®½
            wrapped_row = [textwrap.fill(cell, width=20) for cell in row_data] 
            data.append(wrapped_row)
            
        if not data: return None
        
        df = pd.DataFrame(data, columns=headers)

        # 2. åŠ¨æ€è®¡ç®—å›¾ç‰‡å°ºå¯¸
        # é«˜åº¦ = è¡Œæ•° * ç³»æ•° + æ ‡é¢˜æ 
        # å®½åº¦ = åˆ—æ•° * ç³»æ•°
        row_height = 0.8
        fig_height = len(data) * row_height + 1.5
        fig_width = min(len(headers) * 3, 12) # é™åˆ¶æœ€å¤§å®½åº¦
        
        fig, ax = plt.subplots(figsize=(fig_width, fig_height))
        ax.axis('off')
        
        # 3. ç»˜åˆ¶è¡¨æ ¼
        table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center')
        
        # 4. ä¸“ä¸šæ ·å¼å¾®è°ƒ
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 2) # å¢åŠ è¡Œé«˜ï¼Œè®©æ–‡å­—æ›´èˆ’å±•
        
        for (row, col), cell in table.get_celld().items():
            cell.set_edgecolor('#d0d0d0') # æç»†ç°è¾¹æ¡†
            cell.set_linewidth(0.5)
            
            if row == 0:
                # è¡¨å¤´ï¼šæ·±è‰²å•†åŠ¡è“èƒŒæ™¯ + ç™½å­— + åŠ ç²—
                cell.set_facecolor('#2c3e50')
                cell.set_text_props(color='white', weight='bold', fontsize=11)
            else:
                # å†…å®¹ï¼šéš”è¡Œå˜è‰²
                cell.set_facecolor('#f9f9f9' if row % 2 else '#ffffff')
                cell.set_text_props(color='#333333') # æ·±ç°å­—ä½“

        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', bbox_inches='tight', dpi=300, pad_inches=0.1)
        plt.close(fig)
        img_buffer.seek(0)
        return img_buffer
    except Exception as e:
        print(f"Table Error: {e}")
        return None

def generate_professional_word(content_text, model_name):
    """
    ã€å‡çº§ç‰ˆã€‘Word ç”Ÿæˆå¼•æ“ï¼šMBB å’¨è¯¢é£æ ¼æ’ç‰ˆ
    """
    doc = Document()
    
    # 1. è®¾ç½®é»˜è®¤å­—ä½“ (Calibri / Arial) - æ›´åŠ å•†åŠ¡
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Calibri'
    font.size = Pt(11)
    # å¼ºåˆ¶è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé˜²æ­¢ä¹±ç 
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'SimHei')
    
    # 2. è®¾ç½®æ®µè½é—´è· (é˜²æ­¢æ–‡å­—æŒ¤åœ¨ä¸€èµ·)
    paragraph_format = style.paragraph_format
    paragraph_format.space_after = Pt(8) # æ®µåé—´è·
    paragraph_format.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
    paragraph_format.line_spacing = 1.15 # 1.15å€è¡Œè·
    paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY # ä¸¤ç«¯å¯¹é½ (ä¸“ä¸šå…³é”®)

    # 3. å°é¢/æŠ¬å¤´
    head = doc.add_heading('Investment Research Report', 0)
    head.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    meta = doc.add_paragraph(f"Generated by AI Agent | {datetime.now().strftime('%Y-%m-%d')}")
    meta.alignment = WD_ALIGN_PARAGRAPH.RIGHT
    meta.runs[0].font.color.rgb = RGBColor(100, 100, 100)
    doc.add_paragraph("_" * 50)

    lines = content_text.split('\n')
    inside_table = False
    table_buffer = []

    for line in lines:
        stripped = line.strip()
        # ä¸¥æ ¼çš„è¡¨æ ¼æ£€æµ‹
        is_table_row = stripped.startswith('|') and stripped.endswith('|')
        
        if is_table_row:
            inside_table = True
            table_buffer.append(stripped)
        else:
            # å¦‚æœåˆšæ‰åœ¨è¡¨æ ¼é‡Œï¼Œç°åœ¨å‡ºæ¥äº† -> æ¸²æŸ“è¡¨æ ¼
            if inside_table:
                img = create_professional_table_image(table_buffer)
                if img: 
                    # å±…ä¸­æ’å…¥å›¾ç‰‡
                    p = doc.add_paragraph()
                    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                    run = p.add_run()
                    run.add_picture(img, width=Inches(6.2)) # é€‚åº”A4å®½åº¦
                inside_table = False
                table_buffer = []
            
            # æ¸²æŸ“æ™®é€šæ–‡æœ¬ (å¸¦æ ·å¼)
            if not stripped: continue
            
            if stripped.startswith('# '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 1)
                h.paragraph_format.space_before = Pt(18)
            elif stripped.startswith('## '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 2)
                h.paragraph_format.space_before = Pt(12)
            elif stripped.startswith('### '): 
                h = doc.add_heading(stripped.replace('#','').strip(), 3)
            elif stripped.startswith('- ') or stripped.startswith('* '): 
                p = doc.add_paragraph(stripped[2:], style='List Bullet')
            else:
                # æ­£æ–‡å†…å®¹
                doc.add_paragraph(stripped)

    # å¤„ç†æ–‡æœ«è¡¨æ ¼
    if inside_table and table_buffer:
        img = create_professional_table_image(table_buffer)
        if img: 
            p = doc.add_paragraph()
            p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = p.add_run()
            run.add_picture(img, width=Inches(6.2))
    
    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- UI & Logic ---
with st.sidebar:
    st.title("ğŸ—ƒï¸ å†å²è®°å½•")
    if st.session_state['history']:
        for i, item in enumerate(reversed(st.session_state['history'])):
            if st.button(f"Load: {item['time']}", key=f"hist_{i}"):
                st.session_state['current_report'] = item
                st.rerun()
    
    st.divider()
    api_key = st.text_input("API Key", value="sk-3UIO8MwTblfyQuEZz2WUCzQOuK4QwwIPALVcNxFFNUxJayu7", type="password")
    
    model_name = st.selectbox("Model", ["gemini-3-pro", "gemini-2.5-pro", "qwen-max", "gpt-4o"])
    st.info("ğŸ’ ä¸¥æ ¼æ¨¡å¼å·²å¼€å¯ï¼šæ‰€æœ‰è¡¨æ ¼å°†å¼ºåˆ¶è½¬ä¸ºé«˜æ¸…å›¾ç‰‡ï¼ŒWord æ’ç‰ˆå·²ä¼˜åŒ–ä¸ºå’¨è¯¢çº§æ ¼å¼ã€‚")

st.title("ğŸ’ Pro Research Agent (Perfect Format)")

uploaded_file = st.file_uploader("ä¸Šä¼  PDF èµ„æ–™", type=['pdf'])

if uploaded_file and st.button("ğŸ”¥ å¼€å§‹å®Œç¾è½¬åŒ–"):
    api_url = "https://api.nuwaapi.com/v1/chat/completions"
    
    # 1. è§£æ
    with st.spinner("ğŸ“– è¯»å– PDF (å°è¯•åˆå¹¶è·¨é¡µè¡¨æ ¼)..."):
        raw_text = extract_text_from_pdf(uploaded_file)

    # 2. 1:1 è½¬åŒ–
    chunks = split_text_into_chunks(raw_text, chunk_size=2500)
    full_article_parts = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, chunk in enumerate(chunks):
        status_text.markdown(f"**ğŸ”„ æ ¼å¼åŒ–å¤„ç†ä¸­: Part {i+1}/{len(chunks)}**")
        
        # === æ ¸å¿ƒ Promptï¼šå¼ºåˆ¶è¦æ±‚è¡¨æ ¼å®Œæ•´æ€§ ===
        prompt = """
        You are a Senior Data Entry Specialist. 
        Task: Digitally transcribe the provided document text into Markdown.
        
        **STRICT RULES FOR PERFECT FORMATTING**:
        1. **TABLES**: 
           - If a table spans across pages in the raw text, **MERGE IT** into one single Markdown table.
           - Output valid Markdown tables (| Col1 | Col2 |).
           - DO NOT output broken tables.
        2. **CONTENT**: Word-for-word transcription. No summarization.
        3. **CLEANUP**: Remove headers/footers like "Page 1 of 10".
        """
        msg = [{"role": "user", "content": f"{prompt}\n\nRAW CONTENT:\n{chunk}"}]
        
        chunk_res = None
        for attempt in range(2):
            chunk_res = call_ai_api(api_key, api_url, model_name, msg)
            if chunk_res: break
            time.sleep(1)
        
        if chunk_res:
            full_article_parts.append(chunk_res)
        else:
            full_article_parts.append(f"\n\n[Error processing part {i+1}]\n\n")
            
        progress_bar.progress((i + 1) / len(chunks))

    final_article = "\n\n".join(full_article_parts)
    status_text.success("âœ… æ ¼å¼åŒ–å®Œæˆï¼")

    # 3. ç¤¾åª’ç”Ÿæˆ (Reddit æ·±åº¦ä¼˜åŒ–ç‰ˆ)
    with st.spinner("ğŸ§  æ­£åœ¨æ’°å†™ç¤¾åª’ (å« Reddit DD)..."):
        
        context_head = final_article[:6000]
        context_tail = final_article[-8000:] if len(final_article) > 8000 else ""
        social_context = context_head + "\n\n[...SKIPPING...]\n\n" + context_tail
        
        social_prompt = """
        Act as a Lead Analyst at a Hedge Fund. Write social media content.
        
        **CORE GOAL**: Sell the *Logic* and the *Upside*. Be analytical, not journalistic.
        
        **PLATFORM STRATEGY**:
        
        ### ğŸ”µ LinkedIn (Professional)
        - "The market is missing X about [Company]."
        - 3 Bullet points on Structural Catalysts.
        - Conclusion: Why this is a Buy/Sell now.
        
        ### âš« Twitter/X (Thread)
        - Hook: A chart or number that shocks people.
        - Body: 5 tweets explaining the "Asymmetric Upside".
        - Tone: High conviction.
        
        ### ğŸ”´ Reddit (r/SecurityAnalysis Style DD)
        - **Title**: [DD] [Ticker] - Why the market is wrong about [Topic] (Thesis inside)
        - **Structure**:
          1. **TL;DR**: 2 sentences summary.
          2. **The Thesis**: The main argument.
          3. **The Numbers**: Key valuation metrics (e.g. EV/EBITDA, FCF Yield).
          4. **The Bear Case**: What could go wrong? (Show you are objective).
          5. **Conclusion**: Target price or horizon.
        - **Tone**: Serious, analytical, detailed. No emojis.
        
        ### ğŸŸ  Xiaohongshu
        - Title: âš ï¸è®¤çŸ¥å·®ï¼[Company] çœŸæ­£çš„çˆ†å‘ç‚¹
        - Body: Emoji heavy, focus on "Next Big Thing".
        
        Split with '==='.
        """
        
        msg_social = [{"role": "user", "content": f"{social_prompt}\n\nREPORT:\n{social_context}"}]
        social_res = call_ai_api(api_key, api_url, model_name, msg_social, temperature=0.7)
        if not social_res: social_res = "Generate Failed."

    # 4. ç”Ÿæˆ Word (MBB çº§)
    with st.spinner("ğŸ’¾ æ­£åœ¨æ¸²æŸ“ä¸“ä¸š Word æ–‡æ¡£ (Styles & Images)..."):
        word_bio = generate_professional_word(final_article, model_name)

    # 5. å­˜æ¡£
    report_data = {
        "time": datetime.now().strftime("%H:%M"),
        "filename": uploaded_file.name,
        "article": final_article,
        "social": social_res,
        "word_data": word_bio.getvalue()
    }
    st.session_state['current_report'] = report_data
    st.session_state['history'].append(report_data)
    st.rerun()

# --- ç»“æœ ---
current = st.session_state['current_report']

if current:
    st.divider()
    st.markdown(f"## ğŸ“Š äº¤ä»˜: {current['filename']}")
    col1, col2 = st.columns([5, 5])
    
    with col1:
        st.download_button(
            "ğŸ“¥ ä¸‹è½½ Word (å’¨è¯¢çº§æ’ç‰ˆ+é«˜æ¸…å›¾è¡¨)",
            data=current['word_data'],
            file_name=f"Pro_Report_{current['time']}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        with st.expander("ğŸ“„ åŸå§‹å†…å®¹", expanded=False):
            st.markdown(current['article'])

    with col2:
        st.success("ğŸ”¥ æ·±åº¦ç¤¾åª’æ–‡æ¡ˆ (Reddit DD & Insight)")
        st.text_area("Copy", value=current['social'], height=800)

elif not uploaded_file:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼ ã€‚ç³»ç»Ÿå°†è‡ªåŠ¨æ‰§è¡Œè¡¨æ ¼å®Œæ•´åŒ–ã€æ ·å¼ç¾åŒ–å’Œ Reddit æ·±åº¦æ’°å†™ã€‚")
