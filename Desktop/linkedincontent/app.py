import streamlit as st
import requests
import pdfplumber
import io
import time
import textwrap
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from docx import Document
from docx.shared import Pt, Inches, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
from docx.oxml.ns import qn

# --- å…¨å±€é…ç½® ---
st.set_page_config(page_title="Pro Research Agent (Final)", layout="wide", page_icon="ğŸ’")

# é…ç½®ç»˜å›¾é£æ ¼ (è§£å†³ä¸­æ–‡ä¹±ç å’Œæ ·å¼é—®é¢˜)
plt.style.use('ggplot')
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial', 'DejaVu Sans', 'Microsoft YaHei'] 
plt.rcParams['axes.unicode_minus'] = False

# --- çŠ¶æ€ç®¡ç† ---
if 'history' not in st.session_state:
    st.session_state['history'] = []
if 'current_report' not in st.session_state:
    st.session_state['current_report'] = None

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def extract_pages_from_pdf(uploaded_file):
    """æŒ‰é¡µæå–æ–‡æœ¬ï¼Œä¿è¯è¡¨æ ¼ç»“æ„ä¸è¢«åˆ‡åˆ†"""
    pages_content = []
    with pdfplumber.open(uploaded_file) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                pages_content.append(text)
    return pages_content

def call_ai_api(api_key, base_url, model_name, messages, temperature=0.1, timeout=300):
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model_name, "messages": messages, "temperature": temperature, "stream": False}
    try:
        response = requests.post(base_url, headers=headers, json=payload, timeout=timeout)
        if response.status_code == 200:
            return response.json()['choices'][0]['message']['content']
        else:
            print(f"âš ï¸ API Error: {response.status_code}")
            return None 
    except Exception as e:
        print(f"âš ï¸ Connection Error: {e}")
        return None

def create_professional_table_image(markdown_table_lines):
    """
    ã€ä¿®å¤ç‰ˆã€‘è¡¨æ ¼ç»˜å›¾å¼•æ“ï¼šæ›´å¼ºçš„å®¹é”™æ€§ï¼Œç¡®ä¿è¾“å‡ºå›¾ç‰‡
    """
    try:
        # 1. æ¸…æ´—æ•°æ®ï¼Œå»é™¤æ— å…³çš„åˆ†å‰²çº¿ (å¦‚ |---|)
        clean_rows = []
        for line in markdown_table_lines:
            content = line.strip()
            if not content: continue
            # ç§»é™¤ Markdown è¡¨æ ¼çš„åˆ†å‰²è¡Œ (åŒ…å«å¤§é‡ - æˆ– :)
            if set(content.replace('|', '').strip()) <= {'-', ':', ' '}:
                continue
            clean_rows.append(content)

        if len(clean_rows) < 2: return None # è‡³å°‘è¦æœ‰è¡¨å¤´å’Œä¸€è¡Œæ•°æ®
        
        # 2. è§£æè¡¨å¤´
        headers = [h.strip() for h in clean_rows[0].strip('|').split('|')]
        
        # 3. è§£ææ•°æ®è¡Œ
        data = []
        row_heights = []
        col_width_chars = 20 # ç¨å¾®è°ƒå°æ¢è¡Œå®½åº¦ï¼Œé˜²æ­¢å›¾ç‰‡è¿‡é«˜
        
        for row_line in clean_rows[1:]:
            cells = [c.strip() for c in row_line.strip('|').split('|')]
            
            # å¯¹é½åˆ—æ•° (ä¸è¶³è¡¥ç©ºï¼Œå¤šäº†æˆªæ–­)
            if len(cells) < len(headers):
                cells += [""] * (len(headers) - len(cells))
            elif len(cells) > len(headers):
                cells = cells[:len(headers)]
                
            wrapped_row = []
            max_lines = 1
            for cell_text in cells:
                # è‡ªåŠ¨æ¢è¡Œå¤„ç†
                wrapped = textwrap.fill(cell_text, width=col_width_chars)
                wrapped_row.append(wrapped)
                lines = wrapped.count('\n') + 1
                if lines > max_lines: max_lines = lines
            
            data.append(wrapped_row)
            row_heights.append(max_lines)

        if not data: return None

        df = pd.DataFrame(data, columns=headers)

        # 4. ç»˜å›¾è®¡ç®—
        base_h = 0.5
        header_h = 0.6
        total_h = header_h + sum([rh * base_h for rh in row_heights]) + 0.5
        # åŠ¨æ€å®½åº¦ï¼šåˆ—æ•°è¶Šå¤šè¶Šå®½ï¼Œä½†è®¾ä¸Šé™
        total_w = min(len(headers) * 3, 12) 

        fig, ax = plt.subplots(figsize=(total_w, total_h))
        ax.axis('off')
        
        # 5. ç”Ÿæˆè¡¨æ ¼
        table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='left')
        
        # 6. ç¾åŒ–æ ·å¼
        table.auto_set_font_size(False)
        table.set_fontsize(11)
        cells = table.get_celld()
        
        for (row, col), cell in cells.items():
            cell.set_edgecolor('#cccccc')
            cell.set_linewidth(0.5)
            # è®¾ç½®å†…è¾¹è·
            cell.set_text_props(position=(0.02, cell.get_text_props()['position'][1]))
            
            if row == 0:
                cell.set_height(header_h / total_h)
                cell.set_facecolor('#2c3e50')
                cell.set_text_props(color='white', weight='bold', ha='center')
            else:
                rh_mult = row_heights[row-1]
                cell.set_height((rh_mult * base_h) / total_h)
                cell.set_facecolor('#f8f9fa' if row % 2 else '#ffffff')
                cell.set_text_props(color='black', ha='left', wrap=True)

        img_buffer = io.BytesIO()
        plt.savefig(img_buffer, format='png', bbox_inches='tight', pad_inches=0.1, dpi=300)
        plt.close(fig)
        img_buffer.seek(0)
        return img_buffer

    except Exception as e:
        print(f"Table Gen Error: {e}")
        return None

def generate_professional_word(content_text, model_name):
    """
    ã€ä¿®å¤ç‰ˆã€‘Word ç”Ÿæˆé€»è¾‘ï¼šç¡®ä¿æœ€åä¸€å¼ è¡¨ä¹Ÿèƒ½è¢«å†™å…¥
    """
    doc = Document()
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Calibri'
    font.size = Pt(11)
    style.element.rPr.rFonts.set(qn('w:eastAsia'), 'SimHei')
    
    paragraph_format = style.paragraph_format
    paragraph_format.space_after = Pt(8)
    paragraph_format.line_spacing = 1.15
    
    # æ ‡é¢˜
    head = doc.add_heading('Investment Research Report', 0)
    head.alignment = WD_ALIGN_PARAGRAPH.CENTER
    doc.add_paragraph(f"Generated by AI | {datetime.now().strftime('%Y-%m-%d')}", style='Normal').alignment = WD_ALIGN_PARAGRAPH.RIGHT
    doc.add_paragraph("_" * 50)

    lines = content_text.split('\n')
    inside_table = False
    table_buffer = []

    for line in lines:
        stripped = line.strip()
        
        # åˆ¤å®šè¡¨æ ¼è¡Œï¼šä»¥ | å¼€å¤´å¹¶ä»¥ | ç»“å°¾ (æ”¾å®½ä¸­é—´å†…å®¹çš„é™åˆ¶)
        is_table_row = stripped.startswith('|') and stripped.endswith('|')
        
        if is_table_row:
            inside_table = True
            table_buffer.append(stripped)
        else:
            if inside_table:
                # è¡¨æ ¼ç»“æŸï¼Œç«‹å³å¤„ç†ç¼“å†²åŒº
                img = create_professional_table_image(table_buffer)
                if img:
                    p = doc.add_paragraph()
                    p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                    run = p.add_run()
                    run.add_picture(img, width=Inches(6.5))
                else:
                    # å¦‚æœç»˜å›¾å¤±è´¥ï¼Œå›é€€åˆ°æ–‡æœ¬æ¨¡å¼ï¼Œé˜²æ­¢å†…å®¹ä¸¢å¤±
                    for tb_line in table_buffer:
                        doc.add_paragraph(tb_line, style='Normal')
                
                inside_table = False
                table_buffer = []

            # å¤„ç†éè¡¨æ ¼å†…å®¹
            if not stripped: continue
            
            if stripped.startswith('# '): 
                doc.add_heading(stripped.replace('#','').strip(), 1)
            elif stripped.startswith('## '): 
                doc.add_heading(stripped.replace('#','').strip(), 2)
            elif stripped.startswith('### '): 
                doc.add_heading(stripped.replace('#','').strip(), 3)
            elif stripped.startswith('- ') or stripped.startswith('* '): 
                doc.add_paragraph(stripped[2:], style='List Bullet')
            else:
                doc.add_paragraph(stripped)

    # ã€å…³é”®ä¿®å¤ã€‘å¾ªç¯ç»“æŸåï¼Œæ£€æŸ¥æ˜¯å¦è¿˜é—ç•™äº†ä¸€ä¸ªè¡¨æ ¼åœ¨ç¼“å†²åŒº
    if inside_table and table_buffer:
        img = create_professional_table_image(table_buffer)
        if img:
            p = doc.add_paragraph()
            p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            run = p.add_run()
            run.add_picture(img, width=Inches(6.5))
        else:
            for tb_line in table_buffer:
                doc.add_paragraph(tb_line)

    bio = io.BytesIO()
    doc.save(bio)
    return bio

# --- UI ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ—ƒï¸ å†å²è®°å½•")
    if st.session_state['history']:
        for i, item in enumerate(reversed(st.session_state['history'])):
            if st.button(f"Load: {item['time']}", key=f"hist_{i}"):
                st.session_state['current_report'] = item
                st.rerun()
    st.divider()
    api_key = st.text_input("API Key", value="sk-3UIO8MwTblfyQuEZz2WUCzQOuK4QwwIPALVcNxFFNUxJayu7", type="password")
    model_name = st.selectbox("Model", ["gemini-3-pro", "gpt-4o", "qwen-max"])

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ’ Pro Research Agent (Final Fixed)")

uploaded_file = st.file_uploader("ä¸Šä¼  PDF", type=['pdf'])

if uploaded_file and st.button("ğŸ”¥ å¼€å§‹å®Œç¾è½¬åŒ–"):
    api_url = "https://api.nuwaapi.com/v1/chat/completions"
    
    # 1. é€é¡µè§£æ
    with st.spinner("ğŸ“– é€é¡µè¯»å– PDF..."):
        pages_list = extract_pages_from_pdf(uploaded_file)

    # 2. 1:1 æ•°å­—åŒ– (OCRæ¨¡å¼)
    full_article_parts = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, page_text in enumerate(pages_list):
        status_text.markdown(f"**ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(pages_list)} é¡µ (è¡¨æ ¼é‡æ„ä¸­)...**")
        
        prompt = """
        You are an advanced OCR Engine. 
        Task: Transcribe the text exactly. 
        Rules:
        1. **Formatting**: Use Markdown (# Headers, - Lists).
        2. **Tables**: DETECT TABLES and output them as standard Markdown tables (| Header |... |---|).
        3. **Content**: No summarizing. Word-for-word exact match.
        """
        msg = [{"role": "user", "content": f"{prompt}\n\nCONTENT:\n{page_text}"}]
        
        res = call_ai_api(api_key, api_url, model_name, msg, temperature=0.1)
        
        if res:
            full_article_parts.append(res)
        else:
            full_article_parts.append(f"\n\n{page_text}\n\n") # ä¿åº•
            
        progress_bar.progress((i + 1) / len(pages_list))

    final_article = "\n\n".join(full_article_parts)
    status_text.success("âœ… å†…å®¹ 1:1 æå–å®Œæˆ")

    # 3. ç¤¾åª’ç”Ÿæˆ (æ¢å¤è¯¥åŠŸèƒ½)
    with st.spinner("ğŸ§  æ­£åœ¨æ’°å†™æ·±åº¦ç¤¾åª’ (Lead Analyst Mode)..."):
        social_prompt = """
        Act as a Lead Analyst at a Hedge Fund. 
        Write social media content based on the report provided.
        **GOAL**: Sell the Logic, Catalysts, and Upside. 
        **PLATFORMS**: 
        1. LinkedIn (Professional, bullet points)
        2. Twitter/X (Thread style, catchy)
        3. Reddit (DD style, informal depth)
        
        Split platforms with '==='.
        """
        # æˆªå–å¤´å°¾ä»¥é˜² token æº¢å‡ºï¼Œä½†ä¿ç•™æ ¸å¿ƒ
        context = final_article[:8000] 
        msg_social = [{"role": "user", "content": f"{social_prompt}\n\nREPORT:\n{context}"}]
        social_res = call_ai_api(api_key, api_url, model_name, msg_social, temperature=0.7)
        
        if not social_res: social_res = "âš ï¸ ç¤¾åª’ç”Ÿæˆè¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚"

    # 4. ç”Ÿæˆ Word
    with st.spinner("ğŸ’¾ æ­£åœ¨æ¸²æŸ“ Word (è¡¨æ ¼è½¬å›¾ç‰‡)..."):
        word_bio = generate_professional_word(final_article, model_name)

    # 5. å­˜æ¡£
    report_data = {
        "time": datetime.now().strftime("%H:%M"),
        "filename": uploaded_file.name,
        "article": final_article,
        "social": social_res,
        "word_data": word_bio.getvalue()
    }
    st.session_state['current_report'] = report_data
    st.session_state['history'].append(report_data)
    st.rerun()

# --- ç»“æœå±•ç¤º ---
current = st.session_state['current_report']

if current:
    st.divider()
    st.markdown(f"## ğŸ“Š äº¤ä»˜: {current['filename']}")
    
    col1, col2 = st.columns([4, 6])
    
    with col1:
        st.subheader("ğŸ“¥ æˆæœä¸‹è½½")
        st.download_button(
            "ğŸ’¾ ä¸‹è½½ Word æŠ¥å‘Š (å«è¡¨æ ¼å›¾ç‰‡)",
            data=current['word_data'],
            file_name=f"Report_{current['time']}.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        )
        st.divider()
        st.subheader("ğŸ“‹ 1:1 åŸå§‹å†…å®¹ (ç”¨äºå¤åˆ¶)")
        st.info("ğŸ‘‡ è¿™æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬åŒºåŸŸï¼Œä½ å¯ä»¥å…¨é€‰å¤åˆ¶ï¼Œç²˜è´´åˆ°ä»»ä½•åœ°æ–¹ã€‚å®ƒä¿ç•™äº†æ‰€æœ‰æ–‡å­—å’Œ Markdown ç¬¦å·ã€‚")
        # ã€ä¿®æ”¹ç‚¹ã€‘ä½¿ç”¨ text_area è€Œä¸æ˜¯ codeï¼Œæ–¹ä¾¿æ™®é€šå¤åˆ¶
        st.text_area("Original Content", value=current['article'], height=600)

    with col2:
        st.subheader("ğŸ”¥ æ·±åº¦ç¤¾åª’æ–‡æ¡ˆ (å·²æ¢å¤)")
        # ã€ä¿®æ”¹ç‚¹ã€‘ç¤¾åª’éƒ¨åˆ†å•ç‹¬å±•ç¤ºï¼Œé«˜åº¦è‡ªé€‚åº”
        st.text_area("Social Media Copy", value=current['social'], height=800)

elif not uploaded_file:
    st.info("ğŸ‘ˆ è¯·ä¸Šä¼  PDFã€‚æœ¬ç‰ˆæœ¬å·²å¼ºåˆ¶ä¿®å¤è¡¨æ ¼å›¾ç‰‡ç”Ÿæˆå’Œç¤¾åª’æ–‡æ¡ˆé€»è¾‘ã€‚")
